---
title: "Unit7"
author: "HH"
date: "10/28/2014"
output: html_document
---

# Unit 7: Introduction.

- Multiple predictors.

- Inference for MLR.

- Model Selection.

- Model Diagnostics.

# Unit 7, Part 1: Multiple Predictors.

````{r}
library(DAAG);
data(allbacks);
book_mlr=lm(weight ~ volume + cover, data = allbacks);
summary(book_mlr);

y = 197 + .72*450 - 184.05*0;
y
````

## Interaction variables

Are used if the slopes are not the same. (Beyond the scope of the book)

# Unit 7, Part 1: Adjusted R Squared

````{r}

states = read.csv("http://d396qusza40orc.cloudfront.net/statistics/lec_resources/states.csv");

pov_slr = lm(poverty ~ female_house, data = states);

summary(pov_slr);

pov_mlr = lm(poverty ~ female_house + white, data = states);

summary(pov_mlr);

anova(pov_mlr);

````

To determine if adding a predictor was worthwhile.

${R{adj}^2 = 1 - ( \dfrac{SEE}{STT} * \dfrac{n - 1}{n - k -1} ) }$


````{r}
SSE=339.47;
SST=480.25;
n=51;
k=2;

R2_adj = 1 - (SSE/SST * (n - 1)/(n -k - 1));
R2_adj;

````

When any variable is added to the model ${R^2}$ increases.

But if the added variable doesn't really provide any new information, or is completely unrelated, adjusted ${R^2}$ does not increase.

- k is never negative . adjusted ${ R{adj}^2 < R^2 }$

- ${ R{adj}^2  }$ applies a penalty for the number of predictors included in the model.

- We choose models with higher ${ R{adj}^2  }$

# Unit 7, Part 1: Collinearity and Parsimony.

## Collinearity

  - Two predictor variables are said to be **collinear** when they are correlated with each other.
  
  - Remember: Predictors are also called independent variables, so they should be independent of each other.
  
  - Inclusion of collinear predictors(also called **multicollinearity**) complicates model estimation.
  

## Parsimony

  - Avoid adding predictors associated with each other because often times the addition of such variable brings nothing new to the table.

  - Prefer the simplest best model, i.e. the **parsimonious model**
  
    * Occam's razor: Among competing hypothesis, the one with the fewest assumptions should be selected.

  - Addition of collinear variables can result in biased estimates of the regression parameters.
  
  - While it's impossible to avoid collinearity from arising in observational data, experiments are usually designed to control for correlated predictors.
  
# Unit 7, Part 2: Inference for MLR


```{r}
cognitive = read.csv("http://bit.ly/dasi_cognitive");

cog_full = lm(kid_score ~ mom_hs + mom_iq + mom_work + mom_age, data = cognitive);

summary(cog_full);

````

${H_0: \beta_1 = \beta_2 = ... = \beta_k = 0 }$

${H_A: }$ At least one ${ \beta_i }$ is differente than 0.

    ## F-statistic: 29.7 on 4 and 429 DF,  p-value: <2e-16
    
Since p-value < 0.05, the model as a whole is significant.
  
  - The F test yielding a significant result doesn't mean the model fits the data well, it just means at least one of the ${ \beta }$s i non -zero.
  
  - The F test not yielding a significant result doesn't mean individual variables included in the model are not good predictors of y, it just means that the combination of these variables doesn't yield a good model.
  
## Hypothesis testing for slopes

${ H_0: \beta_1 = 0 }$, when all other variables are included in the model.

${ H_A: \beta_1 \ne 0 }$, when all other variables are included in the model.

    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  19.5924     9.2191    2.13    0.034 *  
    ## mom_hsyes     5.0948     2.3145    2.20    0.028 *  
    ## mom_iq        0.5615     0.0606    9.26   <2e-16 ***
    ## mom_workyes   2.5372     2.3507    1.08    0.281    
    ## mom_age       0.2180     0.3307    0.66    0.510 
    
### t-statistic for the slope:

${ T = \dfrac{b_1 - 0}{SE_{b_1}} }$,  ${ df = n - k - 1 }$

```{r}
# Calculate pvalue for mom_hs:yes
n= 434;
k = 4;
b1 = 5.09482;
SE = 2.3145;

df = n - k -1;
T = (b1 - 0)/SE;
pvalue=pt(T, df= df, lower.tail = FALSE ) * 2;
sprintf("T=%.4f, pvalue=%.4f ",T,pvalue);
````

## Confidence intervals for slope
```{r}
# Calculate confidence interval for mom_work:yes
n= 434;
k = 4;
pe = 2.53718;
se = 2.35067;
conf = 0.95;
df = n - k -1;
tstar = abs(qt( (1 - conf)/2, df = df ));
me = tstar*se;
sprintf("tstar=%.4f, (%.4f, %.4f) ",tstar, pe - me, pe + me);

````
# Unit 7, Part 3: Model Selection

## Backwards elimination - Adjusted ${R^2}$

  1. Start with a full model.
  
  2. Drop one variable at a time and record adjusted ${R^2}$ of each smaller model.
  
  3. Pick the model with the highest increase in adjusted ${R^2}$.
  
  4. Repeat until non of the models yield an increas in adjusted ${R^2}$.

## Backwards elimination - p-value

  1. Start with a full model.
  
  2. Drop the variable with the highest p-value and refit a smaller model.
  
  3. Repeat until all variables left in the model are significant.
  
## Adjusted ${R^2}$ vs p-value

  - p-value: finding significant predictors.
  
  - adjusted ${R^2}$ : more reliable predictions.
  
  - p-value depends on the (somewhat arbitrary) 5% significance level cut off.
  
    * different significance level gives different model.
    
    * Used commonly since it requires fitting fewer models.



## Forward selection - Adjusted ${R^2}$

  1. Start with a single predictor regressions of response vs. each explanatory variable.
  
  2. Pick the model with the highest ${R^2}$.
  
  3. Add the remaining variables one at a time to the existing model, and pick the model with the highest adjusted ${R^2}$.
  
  4. Repeat until the addition of any of the remaining variables does not result in a higher adjusted ${R^2}$.
  
## Forward selection - p-value

  1. Start with single predictor regressions of response vs. each explanatory variable.
  
  2. Pick the variable with the lowest significant p-value.
  
  3. Add the remaining variables one at a time to the existing model, and pick the variable with the lowest significant p-value.

## Final model
```{r}
cognitive = read.csv("http://bit.ly/dasi_cognitive");

cog_final = lm(kid_score ~ mom_hs + mom_iq + mom_work, data = cognitive);

summary(cog_final);

````

# Uni 7, Part 4: Diagnostics for MLR.

## 1. Linear relationships between x and y.

  - Each (numerical) explanatory variable needs to be linearly related to the response variable.
  
  - Check using residuals plots (e vs x).
    
    * Looking for a random scatter around 0.
    
    * Instead of scatterplot of y vs. x: allows for considering the other variables that are also in the model, and not just the bivariate relationship between a given x and y.
    
````{r}

cog_final = lm(kid_score ~ mom_hs + mom_iq + mom_work, data = cognitive);

plot(cog_final$residuals ~ cognitive$mom_iq);

````

## 2. Nearly normal residuals with mean 0

  - Some residuals will be positive and some negative.
  
  - On a residuals plot we look for random scatter of residuals around 0.
  
  - This translates to a nearly normal distribution of residuals centered at 0.
  
  - Check using historgram or normal probability plot.
  
````{r}
hist(cog_final$residuals);
qqnorm(cog_final$residuals);
qqline(cog_final$residuals);

````

## 3. Constant variability of residuals.

  - Residuals should be equally variable for low and high values of the predicted response variable.
  
  - Check using residuals plots of residuals vs. predicted ( e vs ${\hat{y}}$)
  
    * Residuals vs. predicted instead of residuals vs. x because it allows for considering the entire model (with all explanatory variables) at once.
    
    * Residuals randomly scatter in a band with a constant width around 0 ( no fan shape).
    
    * Also worthwhile to view absolute value of residuals vs. predicted to identify unusual observations easily.
    
````{r}
plot(cog_final$residuals ~ cog_final$fitted);
plot(abs(cog_final$residuals) ~ cog_final$fitted);

````

## 4. Independent residuals.

  - Independent residuals -> independent observations.
  
  - If time series structure is suspected check using residuals vs. order of data collection.
  
  - If not think about how the data are sampled.
  
````{r}
plot(cog_final$residuals);

````

    