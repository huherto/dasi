---
title: "Unit6"
author: "HH"
date: "10/21/2014"
output: html_document
---
# Unit 5 

# Introduction

Relationship between two numerical variables.

  * Correlation.
  
  * Modeling.
  
  * Model Diagnostics.
  
  * Inference.
  
# Part 1: Correlation

Describes the strength of the *linear association* between two variables. 

Denoted as R

## Properties

The magnitude (absolute value) of the correlation coefficient measures the strength of the linear association between two numerical variables.

The sign of the correlation coefficient indicates the direction of association.

The correlation coefficient is always between -1 (perfect negative linear association) and 1 (perfect positive linear association).

R = 0 indicates no linear relationship.

The correlation coefficient is unit less, and is not affected by changes in the center or scale of either variable. (such as unit conversions)

The correlation of X with Y is the same as of Y with X.

The correlation coefficient is sensitive to outliers.

# Part 2: Residuals

Difference between the observed and predicted y.

residuals:${ e_i = y_i - \hat{y} }$

# Part 2: Least Squares Line

Why least squares ?

  * Most commonly used.
  
  * Easier to compute.
  
  * In many applications, a residual twice as large as another is more than twice as bad.
  
  
  Least squares line ${ \hat{y} = \beta_0 + \beta_1x}$
  
  ${ x }$: explanatory.
  
  ${ \beta_0 }$: intercept.
  
  ${ \beta_1 }$: slope.

  ${ \hat{y} }$: predicted response.
  
## Point Estimates

  Slope: For each unit increase in x, y is expected to be higher/lower on average by "the slope".

  ${ b_1 = \dfrac{S_y}{S_x}R  }$
  
  Intercept: when x = 0, y is expected to equal "the intercept".
  
  ${ b_0 = \bar{y} - b_1\bar{x} }$ 

# Part 2: Prediction and Extrapolation


## Prediction

  - Using the linear model to predict the value of the response variable for a given value of the explanatory variable is called **prediction**
  
  - Plug in the value of x in the linear model equation.
  

## Extrapolation

  - Applying a model estimate to values outside of the realm of the original data is called **extrapolation**.
  
# Part 2: Conditions for Linear Regression

## Linearity 

  - The relationship between the explanatory and the response variable should be linear.
  
  - There are methods for fitting a model to non-linear relationship exist, but they are outside the scope of the course.
  
  - Check using a scatter plot of the data, or a residuals plot.
  
    * Look for an obvious non-linear pattern. 
  
## Nearly normal residuals

  - Residuals should be nearly normally distributed, centered at 0.
  
  - May not be satisfied if there are unusual observations that don't follow the trend of the rest of the data.
  
  - Check using a histogram or normal probability plot of residuals.
  
    * In the histogram. Is distribution symmetrical ?
    
    * In the histogram. Is distribution centered at zero ?
    
    * In then normal probability plot. Are there some values steering a way from normality?

## Constant variability

  - Variability of points around the least squares line should be roughly constant.
  
  - Implies that the variability of residuals around the 0 line should be roughly constant as well.
  
  - Also called homosedasticity.
  
  - Check using residuals plot.
  
    * Does the variability of the data change at different points of the line ? Fan shape ?
  

  <a href="http://bitly.com/slr_diag">Checking linear regression</a>

# Part 2: ${R^2}$

  - The strength of the fit of a linear model is most commonly evaluated using ${R^2}$.
  
  - Calculated as the square of the correlation coefficient.
  
  - Tells us what percent of variability in the response variable is explained by the model.
  
  - The remainder of the variability is explained by variables not included in the model.
  
  - Always between 0 and 1.
  
# Part 2: Regression with categorical explanatory variables.

Empty summary.

# Part 3: Outliers in regression.

Outliers are points that fall away from the cloud of points.

## Types of outliers

  - Outliers that fall horizontally away from the center of the cloud but don't influence the slope of the regression line are called **leverage points**.
  
  - Outliers that actually influence the slope of the regression line are called **influential points**.
  
    * Usually high leverage points.
    
    * To determine if a point is influential, visualize the regression line with and without the point, and ask: Does the slope of the line change considerably ?
    
# Part 4: Inference for linear regresssion.

## Hypothesis testing for significance of predictor.

Is the explanatory variable a significant predictor of the response variable?

${H_0}$ Nothing is going on. ${H_0: \beta_1 = 0 }$

  The explanatory variable is not a significant predictor of the response variable. i.e no relationship -> slope of the relationship is 0.
  
${H_A}$ Something is going on. ${H_A: \beta_1 \ne 0 }$

  The explanatory variable is a significant predictor of the response variable. i.e. relationship -> slope of the relationship is different than 0.
  
### t statistic

${ T = \dfrac{b_1 - 0}{SE_{b_1} }  }$ with ${df = n - 2 }$ 

## Confidence interval for slope.

${ b_1 \pm t^*_{df}SE_{b_1} }$

````{r}
b1= .9014;
se=0.0963;
tstar = abs(qt(0.025, df = 25));
sprintf("b1=%.4f, tstar=%.4f, (%.4f, %.4f)",b1, tstar, b1-tstar*se, b1+tstar*se);

````

# Part 4: Variability partitioning.

