---
title: "Unit4: Inference for numerical variables"
output: html_document
---

Introduction
-----------------------------
- Comparing two means.
- Bootstrapping.
- Working with small samples.
- Comparing many means.

## Part 1:
### Part 1: Hypothesis Testing for Paired Data

Analyzing paired data:

- When two sets of observations have this special
  correspondence (not independent), they are said to be
  paired.
  
- To analyze paired data, it is often useful to look at the difference
  in outcomes of each pair of observations:
  
  ````
  diff = read score - wirte score
  ````
  
- It is importante that we always substract using a consistent order.

Parameter of interest: ${\mu_{diff}}$

Point estimate: ${\bar{x}_{diff}}$
  
#### Hypotheses for paired means

1. Set the hypotheses:

${H_0: \mu_{diff} = 0}$

${H_A: \mu_{diff} \neq 0}$

2. Calculate point estimate. ${\bar{x}_{diff}}$

3. Check conditions.

    1. Independence: Sample observations must be independent.
    
    2. Sample size/skew: n >= 30, larger if the population is very skewed.
  
4. Draw sampling distribution, shade p-value, calculate test statistic.

    ${Z = \dfrac{x_{diff} - \mu_{diff}}{SE_{\bar{x}_{diff}} } }$

5. Make a decision, and interpret it in context of the research question.

````{r}
mu = 0
mean=-0.545
s = 8.887
n = 200
se = s/(sqrt(n))
z = (mean - mu)/se
pvalue=pnorm(z) * 2
c(se, z, pvalue)
````

### Part 1: Confidence Intervals for Paired Data

Estimated the difference between paired means.

  point estimate ${\pm}$ margin of error
  
  ${ SE_{\bar{x}_{diff}} = \dfrac{s_{diff}}{ \sqrt{n_{diff}} } }$ and
  ${ \bar{x}_{diff}  \pm z * SE_{\bar{x}_{diff}} }$

  ${ \bar{x}_{diff}  \pm z * \dfrac{s_{diff}}{ \sqrt{n_{diff}} } }$


Compare pvalue with ${ \alpha }$
````{r}
z=qnorm((1 - .95)/2)
ci1 = mean - z*se
ci2 = mean + z*se
c(se, z, ci1, ci2) 

````

### Part 1: Comparing Independent Means.
- Confidence intervals.
- Hypothesis tests

#### Estimating the difference between independent means.

Parameter of interest: ${ \mu_1 - \mu_2}$

Point estimate: ${ \bar{x}_1 - \bar{x}_2  }$

Point estimate ${ \pm }$ Margin of Error: ${  (\bar{x}_1 - \bar{x}_2) \pm z* SE_{\bar{x}_1 - \bar{x}_2} }$

Standard error of difference between two independent means:
${  SE_{(\bar{x}_1 - \bar{x}_2)} = \sqrt{ \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} } }$

#### Conditions of independence for comparing two independent means.

1. Independence:

  - Within groups. random and n <= 10% population.
  
  - Between groups. (non-paired)
  
2. Sample size/skew: n1 >= 30 and n2 >= 30.

Example:

````{r}
x1 = 41.8
x2 = 39.4
s1 = 15.14
s2=15.12
n1=505
n2=667
z=1.96
se = sqrt(s1^2/n1 + s2^2/n2)
me = z*se
mean = x1 - x2
sprintf("se=%.4f, me=%.4f, mean=%.4f, ci(%.4f, %.4f)", se, me, mean, mean - me, mean + me)
````

#### Testing for a difference between independence means.

${ H_0: \mu_1 - \mu_2 = 0}$

${ H_A: \mu_1 - \mu_2 \ne 0}$

Point estimate:${ {(\bar{x}_1 - \bar{x}_2)} \tilde{} N(mean=0, SE = 0.89) }$

````{r}
x=seq(-3,3,length=200)
y=dnorm(x,mean=0,sd=0.89)
plot(x,y,type="l",lwd=1,col="blue")

z=(mean - 0)/se
pvalue=(1 - pnorm(z))*2
sprintf("z=%.4f, pvalue=%.4f", z, pvalue)
if (pvalue < .05) {
  print("Reject null hypothesis")
}
````

## Part 2:
### Part 2: Bootstraping

1. Tale a bootstrap sample - random sample taken *with replacement* from the original sample, of the same size as the original sample.

2. Calculate the bootstrap statistic - a statistic such as mean, median, proportion, etc. computed on the bootstrap samples.

3. Repeat steps (1) and (2) many times to create a bootstrap distribution - a distribution of bootstrap statistics.

## Part 3:
### Part 3: t Distribution

A large sample ensures that...

1. The sample distribution of the mean is nearly normal.

2. The estimate of the standar error is reliable. ${ SE = \dfrac{s}{ \sqrt{n}} }$

* When n is small and ${\sigma}$ is unkown(almost always), use the **t distribution** to address the uncertanty of the standard error estimate.

* t distribution is bell shaped, but thicker than the normal.

    + Observations more likely to fall beyond 2 SDs from the mean.
  
    + Extra thick tails helpful for mitigating the effect of a less reliable estimate for the standard error of the sampling distribution.
  
    + Always center at 0 (like the standard normal)
    
    + Has one parameter: **degrees of freedom (df) ** - determines thickness of tails
    
    + Approaches normality as degrees of freedom increases.
    
#### Use the t statistic.

* for inference on a mean when
    + ${ \sigma }$ is unknown.
    + n < 30
* Calculated the same way.
    + ${ T = \dfrac{obs - null}{SE} }$
* p-value (same definition)
    + one or two tail area, based on ${H_A}$
    + using R, applet, or table.
    
### Part 3: Inference for a Small Sample Mean.

#### Estimating the mean (based on a small sample)

point estimate ${ \pm }$ margin of error

${ \bar{x} \pm t_{df}^*SE_\bar{x}}$

${ \bar{x} \pm t_{df}^*\dfrac{s}{\sqrt{n}}}$

Degrees of freedom for t statistics for inference on one sample mean: ${df = n - 1}$

    
${ \bar{x} \pm t_{n - 1}^*\dfrac{s}{\sqrt{n}}}$

````{r}
mean = 52.1;
s = 45.1;
n = 22;
t = abs(qt(0.025, df=n-1));
se = s/sqrt(n)
ci1 = mean - t*se;
ci2 = mean + t*se
sprintf("t=%.4f, se=%.4f, (%.4f, %.4f)", t, se, ci1, ci2);
````

#### Hypothesis testing.

Asume:

  ${H_0: \mu = 30 }$ and ${H_A: \mu \ne 30 }$
  

````{r}
mu = 30;
t = (mean - mu)/se;
pvalue = (1 - pt(t, df = n - 1)) * 2; # Two tails.

sprintf("t=%.4f, pvalue=%.4f, rejectH0=%s", t, pvalue, (pvalue < 0.05));

````

### Part 3: Inference for Comparing Two Small Sample Means.

${ SE = \sqrt{ \dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2} } }$

${ df = min(n_1 - 1, n_2 - 1) }$

#### Confidence intervals

point estimate ${\pm}$ margin of error

${(\bar{x_1} - \bar{x_2} ) \pm t_{df}^* SE_{ (\bar{x_1} - \bar{x_2}) } }$


#### Hypthesis testing

Asume:

${ H_0: \mu_{1} - \mu_{2} = 0  }$

${ H_A: \mu_{1} - \mu_{2} \ne 0  }$

${ T_{df} = \dfrac{obs - null}{ SE } }$

${ T_{df} = \dfrac{(\bar{x_1} - \bar{x_2} ) - (\mu_1 - \mu_2) }{SE_{ (\bar{x_1} - \bar{x_2}) }} }$





````{r}
x1 = 52.1;
s1 = 45.1;
n1 = 22;

x2 = 27.1;
s2 = 26.4;
n2 = 22;

df = min(n1 - 1, n2 - 1);
se = sqrt( s1^2/n1 + s2^2/n2 );
mean = x1 - x2;
t = abs(qt(0.025, df=df))
me = t * se;
ci1 = mean - me;
ci2 = mean + me;
sprintf("mean=%.4f, t=%.4f, se=%.4f, (%.4f, %.4f)", mean, t, se, ci1, ci2);

mu = 0;
t = (mean - mu)/se;
pvalue = (1 - pt(t, df = df)) * 2; # Two tails.

sprintf("t=%.4f, pvalue=%.4f, rejectH0=%s", t, pvalue, (pvalue < 0.05));

````
